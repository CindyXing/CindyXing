{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# llama2 deployment to GKE using TGI on GPU\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_gemma_deployment_on_gke.ipynb\">\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading and deploying llama2 from Meta using vLLM inference server. In this notebook we will deploy and serve llama2 on GPUs.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Deploy and run inference for serving llama2 with vLLM on GPUs.\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\n",
        "\n",
        "Before you use GPUs in GKE, we recommend that you complete the following learning path:\n",
        "\n",
        "Learn about [current GPU version availability](https://cloud.google.com/compute/docs/gpus)\n",
        "\n",
        "Learn about [GPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus)\n",
        "\n",
        "\n",
        "### vLLM\n",
        "\n",
        "vLLM is a fast and easy-to-use library for LLM inference and serving.\n",
        "\n",
        "vLLM is fast with:\n",
        "\n",
        "State-of-the-art serving throughput\n",
        "Efficient management of attention key and value memory with PagedAttention\n",
        "Continuous batching of incoming requests\n",
        "Fast model execution with CUDA/HIP graph\n",
        "Quantization: GPTQ, AWQ, SqueezeLLM, FP8 KV Cache\n",
        "Optimized CUDA kernels\n",
        "\n",
        "To learn more, refer to the [vLLM documentation](https://docs.vllm.ai/en/latest/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 44380,
          "status": "ok",
          "timestamp": 1721086458129,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "855d6b96f291",
        "outputId": "5c58830c-61f7-41cd-e090-c39f39fd7503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "Your current Google Cloud CLI version is: 483.0.0\n",
            "Installing components from version: 483.0.0\n",
            "\n",
            "┌─────────────────────────────────────────────┐\n",
            "│     These components will be installed.     │\n",
            "├────────────────────────┬─────────┬──────────┤\n",
            "│          Name          │ Version │   Size   │\n",
            "├────────────────────────┼─────────┼──────────┤\n",
            "│ gke-gcloud-auth-plugin │   0.5.9 │  4.0 MiB │\n",
            "│ kubectl                │ 1.27.15 │  < 1 MiB │\n",
            "│ kubectl                │ 1.27.15 │ 73.4 MiB │\n",
            "└────────────────────────┴─────────┴──────────┘\n",
            "\n",
            "For the latest full release notes, please visit:\n",
            "  https://cloud.google.com/sdk/release_notes\n",
            "\n",
            "Performing in place update...\n",
            "\n",
            "╔════════════════════════════════════════════════════════════╗\n",
            "╠═ Downloading: gke-gcloud-auth-plugin                      ═╣\n",
            "╠════════════════════════════════════════════════════════════╣\n",
            "╠═ Downloading: gke-gcloud-auth-plugin                      ═╣\n",
            "╠════════════════════════════════════════════════════════════╣\n",
            "╠═ Downloading: kubectl                                     ═╣\n",
            "╠════════════════════════════════════════════════════════════╣\n",
            "╠═ Downloading: kubectl                                     ═╣\n",
            "╠════════════════════════════════════════════════════════════╣\n",
            "╠═ Installing: gke-gcloud-auth-plugin                       ═╣\n",
            "╠════════════════════════════════════════════════════════════╣\n",
            "╠═ Installing: gke-gcloud-auth-plugin                       ═╣\n",
            "╠════════════════════════════════════════════════════════════╣\n",
            "╠═ Installing: kubectl                                      ═╣\n",
            "╠════════════════════════════════════════════════════════════╣\n",
            "╠═ Installing: kubectl                                      ═╣\n",
            "╚════════════════════════════════════════════════════════════╝\n",
            "\n",
            "\n",
            "Update done!\n",
            "\n",
            "Fetching cluster endpoint and auth data.\n",
            "kubeconfig entry generated for autopilot-cluster-1.\n",
            "secret/hf-secret created\n"
          ]
        }
      ],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. Set Hugging Face access token in `HF_TOKEN` field. If you don't already have a \"read\" access token, follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create an access token with \"read\" permission. You can find your existing access tokens in the Hugging Face [Access Token](https://huggingface.co/settings/tokens) page.\n",
        "\n",
        "# @markdown 3. **[Optional]** Set `CLUSTER_NAME` if you want to use your own GKE cluster. If not set, this example will create a standard cluster with 2 NVIDIA L4 GPU accelerators.\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# The HuggingFace token used to download models.\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "assert HF_TOKEN, \"Please set Hugging Face access token in `HF_TOKEN`.\"\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Set up gcloud.\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet\n",
        "\n",
        "# The cluster name to create\n",
        "CLUSTER_NAME = \"autopilot-cluster-1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Use existing GKE cluster or create a new cluster.\n",
        "if CLUSTER_NAME:\n",
        "    ! gcloud container clusters get-credentials {CLUSTER_NAME} --location {REGION}\n",
        "else:\n",
        "    now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    CLUSTER_NAME=f\"gke-gemma-cluster-{now}\"\n",
        "    ! gcloud container clusters create {CLUSTER_NAME} \\\n",
        "        --project={PROJECT_ID} \\\n",
        "        --region={REGION} \\\n",
        "        --workload-pool={PROJECT_ID}.svc.id.goog \\\n",
        "        --release-channel=rapid \\\n",
        "        --num-nodes=4\n",
        "    ! gcloud container node-pools create gpupool \\\n",
        "        --accelerator=type=nvidia-l4,count=2,gpu-driver-version=latest \\\n",
        "        --project={PROJECT_ID} \\\n",
        "        --location={REGION} \\\n",
        "        --node-locations={REGION}-a \\\n",
        "        --cluster={CLUSTER_NAME} \\\n",
        "        --machine-type=g2-standard-24 \\\n",
        "        --num-nodes=1\n",
        "\n",
        "# Create Kubernetes secret for Hugging Face credentials\n",
        "! kubectl create secret generic hf-secret \\\n",
        "    --from-literal=hf_api_token={HF_TOKEN} \\\n",
        "    --dry-run=client -o yaml > hf-secret.yaml\n",
        "\n",
        "! kubectl apply -f hf-secret.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6psJZY_zUDgj",
        "outputId": "03c9db54-2bd5-4bcf-e106-7e3da222246e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deployment.apps/tgi-gemma-deployment1 created\n",
            "service/llm-service1 created\n",
            "Waiting for container to be created...\n",
            "\n",
            "NAME                                     READY   STATUS    RESTARTS   AGE\n",
            "tgi-gemma-deployment-68659f584-xm4cc     1/1     Running   0          6h41m\n",
            "tgi-gemma-deployment1-56d56ccb6d-d9p7q   0/1     Pending   0          1s\n",
            "\n",
            "Downloading artifacts...\n"
          ]
        }
      ],
      "source": [
        "# @title Deploy llama2\n",
        "\n",
        "# @markdown This section deploys llama2 on vLLM.\n",
        "\n",
        "# @markdown Select one of the following model version and size options:\n",
        "\n",
        "K8S_YAML = f\"\"\"\n",
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: llama-deployment\n",
        "spec:\n",
        "  replicas: 1\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: llama-server\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: llama-server\n",
        "        ai.gke.io/model: LLaMA2_7B_Chat\n",
        "        ai.gke.io/inference-server: vllm\n",
        "        examples.ai.gke.io/source: model-garden\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: inference-server\n",
        "        image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240313_0916_RC00\n",
        "        resources:\n",
        "          requests:\n",
        "            cpu: 5\n",
        "            memory: 20Gi\n",
        "            ephemeral-storage: 40Gi\n",
        "            nvidia.com/gpu : 1\n",
        "          limits:\n",
        "            cpu: 5\n",
        "            memory: 20Gi\n",
        "            ephemeral-storage: 40Gi\n",
        "            nvidia.com/gpu : 1\n",
        "        command:\n",
        "        - python\n",
        "        - -m\n",
        "        - vllm.entrypoints.api_server\n",
        "        args:\n",
        "        - --host=0.0.0.0\n",
        "        - --port=7080\n",
        "        - --tensor-parallel-size=1\n",
        "        - --swap-space=16\n",
        "        - --gpu-memory-utilization=0.95\n",
        "        - --max-model-len=2048\n",
        "        - --max-num-batched-tokens=4096\n",
        "        - --disable-log-stats\n",
        "        env:\n",
        "        - name: DEPLOY_SOURCE\n",
        "          value: UI_NATIVE_MODEL\n",
        "        - name: MODEL_ID\n",
        "          value: \"Llama2-7B-chat-001\"\n",
        "        - name: AIP_STORAGE_URI\n",
        "          value: \"gs://vertex-model-garden-public-us/llama2/llama2-7b-chat-hf\"\n",
        "        volumeMounts:\n",
        "        - mountPath: /dev/shm\n",
        "          name: dshm\n",
        "      volumes:\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-accelerator: nvidia-l4\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: llama-service\n",
        "spec:\n",
        "  selector:\n",
        "    app: llama-server\n",
        "  type: ClusterIP\n",
        "  ports:\n",
        "  - protocol: TCP\n",
        "    port: 8000\n",
        "    targetPort: 7080\n",
        "\"\"\"\n",
        "\n",
        "with open(\"llama2.yaml\", \"w\") as f:\n",
        "    f.write(K8S_YAML)\n",
        "\n",
        "! kubectl apply -f llama2.yaml\n",
        "\n",
        "# Wait for container to be created.\n",
        "import time\n",
        "\n",
        "print(\"Waiting for container to be created...\\n\")\n",
        "while True:\n",
        "    shell_output = ! kubectl get pod\n",
        "    container_status = \"\\n\".join(shell_output)\n",
        "    if \"1/1\" in container_status:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "print(container_status)\n",
        "\n",
        "# Wait for downloading artifacts.\n",
        "print(\"\\nDownloading artifacts...\")\n",
        "while True:\n",
        "    shell_output = ! kubectl logs -l app=llama-server\n",
        "    logs = \"\\n\".join(shell_output)\n",
        "    if \"Connected\" in logs:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "print(\"Server is up and running.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "executionInfo": {
          "elapsed": 833,
          "status": "error",
          "timestamp": 1721110422616,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "QDTasPgGW7EG",
        "outputId": "e3b32f63-df8c-4ac4-8873-96b6502c837f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b99d79f66e42>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{command}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generated_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# @title Prediction\n",
        "\n",
        "# @markdown Once the server is up and running, you may send prompts to local server for prediction.\n",
        "\n",
        "import json\n",
        "\n",
        "prompt = \"What are the top 5 most popular programming languages? Please be brief.\"  # @param {type: \"string\"}\n",
        "temperature = 0.40  # @param {type: \"number\"}\n",
        "top_p = 0.1  # @param {type: \"number\"}\n",
        "max_tokens = 250  # @param {type: \"number\"}\n",
        "\n",
        "request = {\n",
        "    \"inputs\": prompt,\n",
        "    \"temperature\": temperature,\n",
        "    \"top_p\": top_p,\n",
        "    \"max_tokens\": max_tokens,\n",
        "}\n",
        "\n",
        "command = f\"\"\"kubectl exec -t $( kubectl get pod -l app=gemma-server1 -o jsonpath=\"{{.items[0].metadata.name}}\" ) -c inference-server -- curl -X POST http://localhost:8000/generate \\\n",
        "   -H \"Content-Type: application/json\" \\\n",
        "   -d '{json.dumps(request)}' \\\n",
        "   2> /dev/null\"\"\"\n",
        "\n",
        "output = !{command}\n",
        "print(\"Output:\")\n",
        "print(json.loads(output[0])[\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbRmgoOZF6es"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continouous charges that may incur.\n",
        "\n",
        "! kubectl delete deployments tgi-gemma-deployment\n",
        "! kubectl delete services llm-service\n",
        "! kubectl delete secrets hf-secret\n",
        "\n",
        "DELETE_CLUSTER = False # @param {type: \"boolean\"}\n",
        "\n",
        "if DELETE_CLUSTER:\n",
        "  ! gcloud container clusters delete {CLUSTER_NAME} \\\n",
        "    --region={REGION} \\\n",
        "    --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_llama2_deployment_on_gke.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
